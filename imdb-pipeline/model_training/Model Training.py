# -*- coding: utf-8 -*-
"""307GP_Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i4D56jdktpT9t5TqmLtRZKY56TdfaRjI
"""

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.preprocessing.sequence import pad_sequences

from google.colab import files
uploaded = files.upload()
df = pd.read_csv("imdb_rating_cleaned.csv")

print("Data preview:")
print(df.head())

df['Boxoffice'] = pd.to_numeric(df['Boxoffice'].replace('Unknown', np.nan), errors='coerce')
if df['Boxoffice'].isna().sum() > 0:
    df['Boxoffice'] = df['Boxoffice'].fillna(df['Boxoffice'].median())

def take_first(x):
    if pd.isna(x):
        return "Unknown"
    parts = [p.strip() for p in str(x).split(',') if p.strip() != '']
    return parts[0] if parts else "Unknown"

df['Director_primary'] = df['Director'].apply(take_first)
df['Writer_primary'] = df['Writer'].apply(take_first)
df['Prod_primary']   = df['Production House'].apply(take_first)

def split_multi(x):
    if pd.isna(x):
        return []
    return [p.strip() for p in str(x).split(',') if p.strip() != '']

multi_cols = ['Genre', 'Languages', 'Tags', 'Country Availability']
for c in multi_cols:
    df[c + "_list"] = df[c].apply(split_multi)

num_cols = ['IMDb Votes', 'Runtime', 'Hidden Gem Score', 'Boxoffice']
target_col = 'IMDb Score'

def build_vocab(series, min_freq=1):
    counts = series.value_counts()
    vocab = ['[PAD]', '[UNK]'] + counts[counts >= min_freq].index.tolist()
    return {v: i for i, v in enumerate(vocab)}

director_map = build_vocab(df['Director_primary'])
writer_map   = build_vocab(df['Writer_primary'])
prod_map     = build_vocab(df['Prod_primary'])

def build_token_map(list_series, min_freq=1):
    from collections import Counter
    c = Counter()
    for lst in list_series:
        c.update(lst)
    tokens = ['[PAD]', '[UNK]'] + [tok for tok, ct in c.items() if ct >= min_freq]
    return {v: i for i, v in enumerate(tokens)}

genre_map   = build_token_map(df['Genre_list'])
lang_map    = build_token_map(df['Languages_list'])
tags_map    = build_token_map(df['Tags_list'])
country_map = build_token_map(df['Country Availability_list'])

def lists_to_sequences(list_series, token_map, max_len):
    unk = token_map.get('[UNK]', 1)
    pad = token_map.get('[PAD]', 0)
    seqs = [[token_map.get(t, unk) for t in lst][:max_len] for lst in list_series]
    return pad_sequences(seqs, padding='post', truncating='post', value=pad, maxlen=max_len)

def series_to_ids(series, mapping):
    unk = mapping.get('[UNK]', 1)
    return series.map(lambda x: mapping.get(x, unk)).astype(np.int32).values

X_num = df[num_cols].fillna(0).values.astype(np.float32)
scaler = StandardScaler()
X_num = scaler.fit_transform(X_num)

X_director = series_to_ids(df['Director_primary'], director_map)
X_writer   = series_to_ids(df['Writer_primary'], writer_map)
X_prod     = series_to_ids(df['Prod_primary'], prod_map)

X_genre_seq   = lists_to_sequences(df['Genre_list'], genre_map, max_len=6)
X_lang_seq    = lists_to_sequences(df['Languages_list'], lang_map, max_len=4)
X_tags_seq    = lists_to_sequences(df['Tags_list'], tags_map, max_len=12)
X_country_seq = lists_to_sequences(df['Country Availability_list'], country_map, max_len=10)

y = df[target_col].values.astype(np.float32)

X_train_ids, X_test_ids, y_train, y_test = train_test_split(
    np.arange(len(df)), y, test_size=0.15, random_state=42)
X_train_ids, X_val_ids, y_train, y_val = train_test_split(
    X_train_ids, y_train, test_size=0.1765, random_state=42)

train_inputs = {
    'num_input': X_num[X_train_ids],
    'director_input': X_director[X_train_ids],
    'writer_input': X_writer[X_train_ids],
    'prod_input': X_prod[X_train_ids],
    'genre_seq': X_genre_seq[X_train_ids],
    'lang_seq': X_lang_seq[X_train_ids],
    'tags_seq': X_tags_seq[X_train_ids],
    'country_seq': X_country_seq[X_train_ids],
}
val_inputs = {
    'num_input': X_num[X_val_ids],
    'director_input': X_director[X_val_ids],
    'writer_input': X_writer[X_val_ids],
    'prod_input': X_prod[X_val_ids],
    'genre_seq': X_genre_seq[X_val_ids],
    'lang_seq': X_lang_seq[X_val_ids],
    'tags_seq': X_tags_seq[X_val_ids],
    'country_seq': X_country_seq[X_val_ids],
}
test_inputs = {
    'num_input': X_num[X_test_ids],
    'director_input': X_director[X_test_ids],
    'writer_input': X_writer[X_test_ids],
    'prod_input': X_prod[X_test_ids],
    'genre_seq': X_genre_seq[X_test_ids],
    'lang_seq': X_lang_seq[X_test_ids],
    'tags_seq': X_tags_seq[X_test_ids],
    'country_seq': X_country_seq[X_test_ids],
}

def emb_size(n_unique):
    return int(min(50, max(4, n_unique // 2)))

num_input = layers.Input(shape=(len(num_cols),), name='num_input')

director_input = layers.Input(shape=(), dtype='int32', name='director_input')
writer_input   = layers.Input(shape=(), dtype='int32', name='writer_input')
prod_input     = layers.Input(shape=(), dtype='int32', name='prod_input')

genre_seq   = layers.Input(shape=(X_genre_seq.shape[1],), dtype='int32', name='genre_seq')
lang_seq    = layers.Input(shape=(X_lang_seq.shape[1],), dtype='int32', name='lang_seq')
tags_seq    = layers.Input(shape=(X_tags_seq.shape[1],), dtype='int32', name='tags_seq')
country_seq = layers.Input(shape=(X_country_seq.shape[1],), dtype='int32', name='country_seq')

def embed_single(inp, vocab_map, name):
    emb = layers.Embedding(input_dim=len(vocab_map), output_dim=emb_size(len(vocab_map)), name=name)(inp)
    return layers.Reshape((emb.shape[-1],))(emb)

def embed_multi(inp, vocab_map, name):
    emb = layers.Embedding(input_dim=len(vocab_map), output_dim=emb_size(len(vocab_map)),
                            mask_zero=True, name=name)(inp)
    return layers.GlobalAveragePooling1D()(emb)

director_emb = embed_single(director_input, director_map, 'director_emb')
writer_emb   = embed_single(writer_input, writer_map, 'writer_emb')
prod_emb     = embed_single(prod_input, prod_map, 'prod_emb')

genre_emb   = embed_multi(genre_seq, genre_map, 'genre_emb')
lang_emb    = embed_multi(lang_seq, lang_map, 'lang_emb')
tags_emb    = embed_multi(tags_seq, tags_map, 'tags_emb')
country_emb = embed_multi(country_seq, country_map, 'country_emb')

conc = layers.Concatenate()([num_input,
                             director_emb, writer_emb, prod_emb,
                             genre_emb, lang_emb, tags_emb, country_emb])

x = layers.Dense(256, activation='relu')(conc)
x = layers.Dropout(0.25)(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(64, activation='relu')(x)
output = layers.Dense(1, activation='linear')(x)

model = Model(inputs=[num_input, director_input, writer_input, prod_input,
                      genre_seq, lang_seq, tags_seq, country_seq],
              outputs=output)

model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),
              loss='mse',
              metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae'])

model.summary()

BATCH_SIZE = 64
def make_dataset(inputs_dict, labels, shuffle=False):
    ds = tf.data.Dataset.from_tensor_slices((inputs_dict, labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(labels))
    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

train_ds = make_dataset(train_inputs, y_train, shuffle=True)
val_ds = make_dataset(val_inputs, y_val)
test_ds = make_dataset(test_inputs, y_test)

callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5', save_best_only=True, monitor='val_loss')
]

history = model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=callbacks)

print("\nTest Evaluation:")
model.evaluate(test_ds)

model.save("saved_model.keras", include_optimizer=False)
print("Model saved to 'saved_model.keras'")

import pickle
preproc = {
    'director_map': director_map,
    'writer_map': writer_map,
    'prod_map': prod_map,
    'genre_map': genre_map,
    'lang_map': lang_map,
    'tags_map': tags_map,
    'country_map': country_map,
    'num_cols': num_cols,
    'scaler_mean': scaler.mean_.tolist(),
    'scaler_var': scaler.var_.tolist(),
    'scaler_scale': scaler.scale_.tolist()
}
with open('preproc.pkl', 'wb') as f:
    pickle.dump(preproc, f)

print("Preprocessing artifacts saved to preproc.pkl")

import matplotlib.pyplot as plt
history_data = history.history
epochs = range(1, len(history_data["loss"]) + 1)

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(epochs, history_data["loss"], label='Training Loss')
plt.plot(epochs, history_data["val_loss"], label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)

plt.subplot(1,2,2)
plt.plot(epochs, history_data["mae"], label='Training MAE')
plt.plot(epochs, history_data["val_mae"], label='Validation MAE')
plt.xlabel("Epoch")
plt.ylabel("Mean Absolute Error")
plt.title("Training vs Validation MAE")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()